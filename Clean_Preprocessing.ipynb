{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyP08IYV42Sa9qLRrYI1aDSi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Imports and Dependencies"],"metadata":{"id":"rOODU0oFLKve"}},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git"],"metadata":{"id":"ch7K5DaCMFL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gzip\n","import numpy as np\n","import pandas as pd\n","import numpy as np\n","import torch\n","import clip\n","from sklearn.metrics.pairwise import cosine_similarity\n","import string\n","from collections import Counter\n","import json\n","import os\n","import re\n","from sklearn.decomposition import PCA\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import pickle\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import pickle\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import matplotlib.pyplot as plt\n","from collections import defaultdict\n","from multiprocessing import Pool"],"metadata":{"id":"9vgUVlAKMAsY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"jjS-aJ_MLOaF"}},{"cell_type":"markdown","source":["Please replace `your_path` in the cell below witht the path to your folder with all the data and the annotations.\n","\n","For the COCO Dataset, you can access this [here](https://drive.google.com/drive/folders/1UNxO77KFFxgAG8G4YX_GTFVHzl96-s2T?usp=sharing).\n","\n","For the Viz-Wiz Captions Dataset, you can access this [here](https://drive.google.com/drive/folders/1gZM43wBNRhKsUwJgCJTdR95nzXccJLbn?usp=sharing).\n","\n","Additionally, you may change `output_path` to match the path where you would like to store the embeddings. For the COCO dataset, we recommend you match the labels with the images first before proceeding, as only a subset of the original training dataset is present in the folder (see commented code block below)."],"metadata":{"id":"wSURNA4TLR2a"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","your_path = \"path/to/folder\"\n","metadata_path = os.path.join(your_path, \"6.8611 Final Project/Viz-Wiz Captions/val.json\")\n","Viz_Wiz_images_dir = os.path.join(your_path, \"6.8611 Final Project/Viz-Wiz Captions/val\")\n","output_path = os.path.join(your_path, \"6.8611 Final Project/Viz-Wiz Captions/viz_wiz_embeddings.npz\")"],"metadata":{"id":"WrblivOsMRTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","with open(metadata_path, 'r') as f:\n","    viz_wiz_data = json.load(f)\n","\n","images = {img['id']: img for img in viz_wiz_data['images']}\n","annotations = viz_wiz_data['annotations']\n","print(f\"Loaded {len(images)} images and {len(annotations)} annotations.\")"],"metadata":{"id":"BLUMv9RgNT_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# existing_image_ids = set()\n","# for file in os.listdir(COCO_images_dir):\n","#     if os.path.isfile(os.path.join(COCO_images_dir, file)):\n","#         match = re.match(r\"^(\\d+)\", file) #regex match\n","#         if match:\n","#             image_id = int(match.group(1))\n","#             existing_image_ids.add(image_id)\n","\n","# print(f\"Found {len(existing_image_ids)} valid image IDs in {COCO_images_dir}.\")\n","\n","# filtered_annotations = [ann for ann in annotations if ann['image_id'] in existing_image_ids]\n","# print(f\"Filtered {len(filtered_annotations)} annotations matching existing images.\")"],"metadata":{"id":"GufquPEETo-X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# CLIP Embedding Generation"],"metadata":{"id":"UA6-tHU7OGaL"}},{"cell_type":"markdown","source":["Below are the steps to generate the CLIP embeddings for both the images and their associated captions. Make sure to edit `batch_size` as you want it, in order to match the memory capacities you may have access to."],"metadata":{"id":"KuaMDbZaOLI4"}},{"cell_type":"code","source":["def save_incremental(image_features, text_features, image_ids, text_captions, output_path):\n","    if os.path.exists(output_path):\n","        existing_data = np.load(output_path)\n","        image_features = np.concatenate([existing_data['image_features'], image_features], axis=0)\n","        text_features = np.concatenate([existing_data['text_features'], text_features], axis=0)\n","        image_ids = np.concatenate([existing_data['image_ids'], image_ids], axis=0)\n","        text_captions = np.concatenate([existing_data['text_captions'], text_captions], axis=0)\n","\n","    # Save updated data\n","    np.savez_compressed(\n","        output_path,\n","        image_features=image_features,\n","        text_features=text_features,\n","        image_ids=image_ids,\n","        text_captions=text_captions\n","    )\n","    print(f\"Incrementally saved {len(image_features)} entries to {output_path}.\")"],"metadata":{"id":"ntx50q2jQpmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zavYGLQdLK5m"},"outputs":[],"source":["batch_size = 1000\n","image_features, text_features, image_ids, text_captions = [], [], [], []\n","\n","accumulated_image_features, accumulated_text_features, accumulated_image_ids, accumulated_text_captions = [], [], [], []\n","\n","\n","# Process annotations\n","for i, annotation in enumerate(tqdm(filtered_annotations, desc=\"Extracting Features\")):\n","    image_id = annotation['image_id']\n","    caption = annotation['caption']\n","    image_path = os.path.join(Viz_Wiz_images_dir, images[image_id][\"file_name\"])\n","\n","    # Image\n","    try:\n","        image = preprocess(Image.open(image_path).convert(\"RGB\")).unsqueeze(0).to(device)\n","        image_feature = clip_model.encode_image(image)\n","        image_feature = image_feature / image_feature.norm(dim=-1, keepdim=True)  # Normalize\n","    except Exception as e:\n","        print(f\"Error processing image {image_path}: {e}\")\n","        continue\n","\n","    # Text\n","    tokenized_text = clip.tokenize([caption]).to(device)\n","    text_feature = clip_model.encode_text(tokenized_text)\n","    text_feature = text_feature / text_feature.norm(dim=-1, keepdim=True)  # Normalize\n","\n","    # Store embeddings in accumulated\n","    accumulated_image_features.append(image_feature.detach().numpy())\n","    accumulated_text_features.append(text_feature.detach().numpy())\n","    accumulated_image_ids.append(image_id)\n","    accumulated_text_captions.append(caption)\n","\n","    if (i + 1) % batch_size == 0 or (i + 1) == len(annotations):\n","        save_incremental(\n","            np.array(accumulated_image_features),\n","            np.array(accumulated_text_features),\n","            np.array(accumulated_image_ids),\n","            np.array(accumulated_text_captions),\n","            output_path\n","        )\n","        print(f\"Batch {(i+1)//batch_size} saved.\")\n","        accumulated_image_features, accumulated_text_features, accumulated_image_ids, accumulated_text_captions = [], [], [], []\n","\n","print(\"All batches processed and saved.\")"]},{"cell_type":"markdown","source":["# KG Embedding Generation"],"metadata":{"id":"qqR9-XZUO5_1"}},{"cell_type":"markdown","source":["## Vocabulary and IDF"],"metadata":{"id":"ljLCG8BuO9Yf"}},{"cell_type":"markdown","source":["### Vocabulary"],"metadata":{"id":"_lCs_HgtVM6a"}},{"cell_type":"code","source":["# COCO Captions\n","coco_embeddings_path = os.path.join(your_path, f\"{project_folder}/COCO/coco_embeddings.npz\")\n","coco_data = np.load(coco_embeddings_path, allow_pickle=True)\n","coco_captions = coco_data[\"text_captions\"]\n","\n","#Viz-Wiz Captions\n","viz_wiz_annotations_path = \"6.8611 Final Project/Viz-Wiz Captions/val.json\"\n","total_wiz_annotations_path = os.path.join(you_path, viz_wiz_annotations_path)\n","with open(total_wiz_annotations_path, 'r') as f:\n","    viz_wiz_data = json.load(f)\n","viz_wiz_captions = [ann['caption'] for ann in viz_wiz_data['annotations']]"],"metadata":{"id":"ARpdw3itO_rX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = []\n","total_captions = coco_captions.tolist() + viz_wiz_captions\n","for caption in total_captions:\n","    tokens.extend(normalize_token(word) for word in caption.split())\n","vocabulary = set(tokens)\n","word_to_idx = {word: idx for idx, word in enumerate(sorted(vocabulary))}"],"metadata":{"id":"i3j6Zjw2P32B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocabulary_path = os.path.join(your_path, \"6.8611 Final Project/COCO/our_vocabulary.pkl\")\n","with open(vocabulary_path, 'wb') as f:\n","    pickle.dump(word_to_idx, f)\n","print(\"Vocabulary saved!\")"],"metadata":{"id":"mteCRK1qQeDQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IDF"],"metadata":{"id":"vqe2PNhPVLgK"}},{"cell_type":"markdown","source":["Make sure you save the IDF values for both COCO and Viz-Wiz Captions."],"metadata":{"id":"A7l6ZE0BX_0s"}},{"cell_type":"code","source":["def calculate_idf(captions, output_path):\n","    num_documents = len(captions)\n","    word_document_counts = Counter()\n","\n","    for caption in captions:\n","        tokens = set(normalize_token(word) for word in caption.split())\n","        word_document_counts.update(tokens)\n","\n","    idf = {word: np.log(num_documents / (1 + count)) for word, count in word_document_counts.items()}\n","    idf_df = pd.DataFrame({'word': idf.keys(), 'idf': idf.values()})\n","    idf_df.to_csv(output_path, index=False)\n","    print(f\"IDF values saved to {output_path}\")\n","    return idf\n","\n","idf_values_path = os.path.join(your_path, \"6.8611 Final Project/Viz-Wiz Captions/idf_values.csv\")\n","calculate_idf(viz_wiz_captions, idf_values_path)"],"metadata":{"id":"YXbi51jlU77_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ConceptNet Numberbatch"],"metadata":{"id":"xhMFUV3VQhM2"}},{"cell_type":"code","source":["embeddings_path = os.path.join(your_path, '6.8611 Final Project/ConceptNet_Data_Container/numberbatch-en-19.08.txt.gz')\n","\n","def load_numberbatch_embeddings(file_path):\n","    embeddings = []\n","    terms = []\n","\n","    with gzip.open(file_path, 'rt', encoding='utf8') as f:\n","        next(f)\n","        for line in f:\n","            elements = line.strip().split()\n","            term = elements[0]\n","            vector = list(map(float, elements[1:]))\n","            terms.append(term)\n","            embeddings.append(vector)\n","\n","    return pd.DataFrame(embeddings, index=terms)\n","\n","numberbatch_df = load_numberbatch_embeddings(embeddings_path)"],"metadata":{"id":"FsR5bPMtQfwI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## TF-IDF"],"metadata":{"id":"5Qbnc7fOSdml"}},{"cell_type":"code","source":["class KGEmbedding:\n","    def __init__(self, numberbatch_path, idf_path, device=\"cuda\"):\n","        self.device = device\n","        self.numberbatch_embeddings = self.load_numberbatch(numberbatch_path)\n","        self.idf_dict = self.load_idf(idf_path)\n","\n","    def load_numberbatch(self, numberbatch_path):\n","        return pd.read_csv(\n","            numberbatch_path,\n","            sep=' ',\n","            header=None,\n","            index_col=0,\n","            skiprows=1\n","        )\n","\n","    def normalize_token(self, token):\n","        return token.strip(string.punctuation).lower()\n","\n","    def calculate_tf(self, caption):\n","        tokens = [self.normalize_token(word) for word in caption.split()]\n","        total_tokens = len(tokens)\n","        token_counts = Counter(tokens)\n","        tf = {word: count / total_tokens for word, count in token_counts.items()}\n","        return tf\n","\n","    def calculate_tf_idf(self, caption, idf):\n","        tf = self.calculate_tf(caption)\n","        tf_idf = {word: float(tf_val) * idf.get(word, 0) for word, tf_val in tf.items()}\n","        return tf_idf\n","\n","    def load_idf(self, idf_path):\n","        idf_df = pd.read_csv(idf_path)\n","        idf_dict = dict(zip(idf_df['word'], idf_df['idf']))\n","        return idf_dict\n","\n","    def get_embeddings_for_caption(self, text_caption):\n","        embeddings = []\n","        weights = []\n","        words = text_caption.split()\n","        weight_dict = self.calculate_tf_idf(text_caption, self.idf_dict)\n","        for word in words:\n","            word = self.normalize_token(word)\n","            if word in numberbatch_df.index:\n","                vanilla_embedding = self.numberbatch_embeddings.loc[word].values\n","                weight = weight_dict.get(word, 1.0)\n","                embeddings.append(vanilla_embedding * weight)\n","                weights.append(weight)\n","        if not embeddings:\n","            return np.zeros(self.numberbatch_embeddings.shape[1])\n","\n","        embeddings = np.array(embeddings)\n","        weights = np.array(weights)\n","        weighted_embedding = np.sum(embeddings, axis=0) / np.sum(weights)\n","        return weighted_embedding\n"],"metadata":{"id":"rmGFaHO3QrLf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Run this for both COCO and for Viz-Wiz Captions and update the paths accordingly to store the embeddings. OUr resulting embeddings are in the folders that we linked earlier in the notebook."],"metadata":{"id":"RQXD8ABsXyg_"}},{"cell_type":"code","source":["regenerate_kg = False\n","if regenerate_kg:\n","  viz_wiz_images_dir = f\"{project_folder}/Viz-Wiz Captions/val\"\n","  viz_wiz_annotations_path = f\"{project_folder}/Viz-Wiz Captions/val.json\"\n","\n","  total_wiz_images_dir = os.path.join(ananya_path, viz_wiz_images_dir)\n","  total_wiz_annotations_path = os.path.join(ananya_path, viz_wiz_annotations_path)\n","\n","  with open(total_wiz_annotations_path, 'r') as f:\n","      viz_wiz_data = json.load(f)\n","\n","  images = {img['id']: img for img in viz_wiz_data['images']}\n","  annotations = viz_wiz_data['annotations']\n","  print(f\"Loaded {len(images)} images and {len(annotations)} annotations.\")\n","  viz_wiz_captions = [ann['caption'] for ann in annotations]\n","  print(f\"Loaded {len(viz_wiz_captions)} captions.\")\n","  # set up KG embedding obj\n","  idf_values_path = os.path.join(ananya_path, f\"{project_folder}/Viz-Wiz Captions/idf_values.csv\")\n","  numberbatch_embeddings_path = os.path.join(ananya_path, f'{project_folder}/ConceptNet_Data_Container/numberbatch-en-19.08.txt.gz')\n","  kg_embedding_obj = KGEmbedding(numberbatch_embeddings_path, idf_values_path)\n","  kg_embeddings = {}\n","  for ix, caption in enumerate(viz_wiz_captions):\n","      kg_embedding = kg_embedding_obj.get_embeddings_for_caption(caption)\n","      kg_embeddings[caption] = kg_embedding\n","      if ix % 1000 == 0:\n","          print(f\"Processed {ix} captions.\")\n","  kg_embeddings = torch.tensor(kg_embeddings).float().to(device)\n","  print(len(viz_wiz_captions))\n","  print(len(kg_embeddings))\n","\n","  kg_coco_embedding_path = os.path.join(ananya_path, f\"{project_folder}/Viz-Wiz Captions/kg_viz_wiz_embeddings.npz\")\n","  np.savez_compressed(kg_coco_embedding_path, captions=viz_wiz_captions, kg_embeddings=kg_embeddings.cpu())"],"metadata":{"id":"lPUsS6azSxU2"},"execution_count":null,"outputs":[]}]}