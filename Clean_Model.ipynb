{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jUNSqYNb_eNVVxN4Jp7gm6CJ6MbRpIwR","timestamp":1733295278625},{"file_id":"1ev_aFsz1aEF6oV6cLN-6lDszfyicpr6u","timestamp":1733262991504}],"gpuType":"A100","collapsed_sections":["KHWgUAdn5I7A"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports and Dependencies"],"metadata":{"id":"-kckaHDM-qdS"}},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git\n","!pip install rouge-score\n","!pip install git+https://github.com/salaniz/pycocoevalcap.git"],"metadata":{"id":"u9wx1Lp_itZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","import re\n","import gzip\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","# clip\n","import clip\n","# torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","# eval\n","from multiprocessing import Pool\n","from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","from pycocoevalcap.cider.cider import Cider\n","from pycocoevalcap.spice.spice import Spice\n","from scipy.stats import ttest_ind, sem\n","# misc\n","from sklearn.metrics.pairwise import cosine_similarity\n","import string\n","from collections import Counter\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","from collections import defaultdict"],"metadata":{"id":"b0DhoqTOiQPZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Path"],"metadata":{"id":"6USGh22T_hYG"}},{"cell_type":"markdown","source":["Please replace `your_path` in the cell below witht the path to your folder with all the data and the annotations."],"metadata":{"id":"_sL6yXXiRINi"}},{"cell_type":"code","source":["# your_path = \"path/to/folder\"\n","your_path = \"drive/MyDrive/MIT/3_JuniorYear/6.8611\" # julia's path\n","project_folder = \"6.8611_Final_Project\" # Update to yours"],"metadata":{"id":"8f0kfUw0_iO4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mount gdrive for data access\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"HOk4s10uRsQt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Our Model"],"metadata":{"id":"j1VM6V8g-wlT"}},{"cell_type":"code","source":["class LightweightCaptioningModel(nn.Module):\n","    def __init__(self, clip_dim, kg_dim, hidden_dim, vocab_size, use_kg=False, num_layers=1):\n","        super(LightweightCaptioningModel, self).__init__()\n","        self.use_kg = use_kg\n","\n","        # Image feature projection\n","        self.clip_img_proj = nn.Linear(clip_dim, hidden_dim)\n","\n","        # Word embedding for decoder\n","        self.word_embedding = nn.Embedding(vocab_size, hidden_dim)\n","\n","        # Optional KG projection\n","        if use_kg:\n","            self.kg_proj = nn.Linear(kg_dim, hidden_dim)\n","            self.feature_combine = nn.Linear(hidden_dim * 2, hidden_dim)\n","\n","        # Decoder LSTM\n","        self.decoder_rnn = nn.LSTM(hidden_dim * 2, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, clip_image_embedding, kg_embedding=None, captions=None, lengths=None, **kwargs):\n","        # Process image features\n","        img_features = F.relu(self.clip_img_proj(clip_image_embedding))\n","\n","        # Combine with KG features if using\n","        if self.use_kg and kg_embedding is not None:\n","            kg_features = F.relu(self.kg_proj(kg_embedding))\n","            projected_features = F.relu(self.feature_combine(torch.cat([img_features, kg_features], dim=-1)))\n","        else:\n","            projected_features = img_features\n","\n","        if captions is not None:  # Training mode\n","            return self.training_step(projected_features, captions, lengths)\n","\n","        else:  # Inference mode\n","            return self.evaluate(img_features, **kwargs)\n","\n","    def training_step(self, projected_features, captions, lengths=None):\n","          caption_embeddings = self.word_embedding(captions[:, :-1])\n","          batch_size, seq_len, _ = caption_embeddings.size()\n","\n","          # Repeat visual features for each timestep\n","          repeated_features = projected_features.unsqueeze(1).expand(-1, seq_len, -1)\n","          decoder_input = torch.cat([caption_embeddings, repeated_features], dim=-1)\n","\n","          # Pack sequence for RNN\n","          packed_input = nn.utils.rnn.pack_padded_sequence(\n","              decoder_input, lengths-1, batch_first=True, enforce_sorted=False\n","          )\n","\n","          # Run through decoder\n","          packed_output, _ = self.decoder_rnn(packed_input)\n","          outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n","          logits = self.output_layer(outputs)\n","\n","          target_captions = captions[:, 1:]\n","\n","          return logits, target_captions\n","\n","    def evaluate(self, img_features, idx_to_word, numberbatch_df, idf_values, word_to_idx, max_length=20, beam_width=3):\n","        batch_size = img_features.size(0)\n","        vocab_size = len(word_to_idx)\n","        outputs = []\n","\n","        for batch_idx in range(batch_size):\n","            beams = [(torch.tensor([word_to_idx[\"<s>\"]], device=img_features.device), 0.0)]\n","            completed_sequences = []\n","            hidden_state = None\n","            curr_img_features = img_features[batch_idx].unsqueeze(0)\n","\n","            for step in range(max_length - 1):\n","                candidates = []\n","\n","                for beam_idx, (seq, score) in enumerate(beams):\n","                    if seq[-1].item() == word_to_idx[\"</s>\"]:\n","                        completed_sequences.append((seq, score))\n","                        continue\n","\n","                    # Get word embedding and combine with image features\n","                    word_embedding = self.word_embedding(seq[-1])\n","\n","                    if self.use_kg:\n","                        # Compute KG embeddings for the current sequence\n","                        generated_words = [idx_to_word.get(idx.item(), \"<UNK>\") for idx in seq]\n","                        kg_embedding = self.compute_caption_tfidf_embedding(\n","                            generated_words, numberbatch_df, idf_values\n","                        )\n","                        kg_embedding = torch.tensor(kg_embedding, dtype=torch.float32, device=img_features.device)\n","                        kg_features = F.relu(self.kg_proj(kg_embedding))\n","                        projected_features = F.relu(self.feature_combine(\n","                            torch.cat([curr_img_features, kg_features.unsqueeze(0)], dim=-1)\n","                        ))\n","                    else:\n","                        projected_features = curr_img_features\n","\n","                    # Run through decoder\n","                    decoder_input = torch.cat([word_embedding.unsqueeze(0), projected_features], dim=-1)\n","                    output, new_hidden = self.decoder_rnn(decoder_input.unsqueeze(0), hidden_state)\n","                    word_logits = self.output_layer(output.squeeze(0))\n","\n","                    # Apply repetition penalty\n","                    prev_tokens = set(seq.tolist())\n","                    repetition_penalty = torch.ones_like(word_logits)\n","                    for token_idx in prev_tokens:\n","                        repetition_penalty[0, token_idx] = 0.5\n","\n","                    word_logits = word_logits * repetition_penalty\n","                    word_probs = F.log_softmax(word_logits, dim=-1)\n","\n","                    # Get top k candidates\n","                    values, indices = word_probs.topk(beam_width * 2)  # Get more candidates to account for filtering\n","\n","                    indices = indices.squeeze()\n","                    values = values.squeeze()\n","\n","                    # Track number of candidates added for this beam\n","                    candidates_added = 0\n","                    for value_idx in range(len(values)):\n","                        next_token = indices[value_idx].item()\n","\n","                        # Skip if token index is invalid or token doesn't exist\n","                        if next_token >= vocab_size or next_token not in idx_to_word:\n","                            continue\n","\n","                        token = idx_to_word[next_token]\n","\n","                        # Skip special tokens\n","                        if token in [\"<PAD>\", \"<s>\", \"</s>\", \"<UNK>\"]:\n","                            continue\n","\n","                        # Skip for trigam blocking\n","                        if self._has_excessive_repetition(seq, next_token):\n","                            continue\n","\n","                        value = values[value_idx]\n","                        candidate_seq = torch.cat([seq, indices[value_idx].unsqueeze(0)])\n","\n","                        # Length normalization for score\n","                        length_penalty = ((5.0 + len(candidate_seq)) / 6.0) ** 0.65\n","                        candidate_score = (score + value.item()) / length_penalty\n","\n","                        candidates.append((candidate_seq, candidate_score))\n","                        candidates_added += 1\n","\n","                        # Break if we have enough candidates for this beam\n","                        if candidates_added >= beam_width:\n","                            break\n","\n","                # Keep top candidates that aren't complete\n","                incomplete_candidates = [cand for cand in candidates\n","                                      if cand[0][-1].item() != word_to_idx[\"</s>\"]]\n","\n","                # Update beams\n","                if incomplete_candidates:\n","                    incomplete_candidates.sort(key=lambda x: x[1], reverse=True)\n","                    beams = incomplete_candidates[:beam_width]\n","                    for seq, score in beams:\n","                        beam_tokens = [idx_to_word[idx.item()] for idx in seq]\n","                else:\n","                    break\n","\n","            # Add any remaining sequences\n","            completed_sequences.extend(beams)\n","\n","            # Get best sequence\n","            if completed_sequences:\n","                completed_sequences.sort(key=lambda x: x[1], reverse=True)\n","                best_seq = completed_sequences[0][0]\n","            else:\n","                best_seq = beams[0][0]\n","\n","            best_tokens = [idx_to_word[idx.item()] for idx in best_seq]\n","\n","            # Pad sequence as needed\n","            seq_len = best_seq.size(0)\n","            if seq_len < max_length:\n","                padding = torch.full((max_length - seq_len,), word_to_idx[\"<PAD>\"],\n","                                  dtype=best_seq.dtype, device=best_seq.device)\n","                best_seq = torch.cat([best_seq, padding])\n","            else:\n","                best_seq = best_seq[:max_length]\n","\n","            outputs.append(best_seq)\n","\n","            final_tokens = [idx_to_word[idx.item()] for idx in best_seq]\n","\n","        return torch.stack(outputs)\n","\n","    def _has_excessive_repetition(self, seq, next_token):\n","        if len(seq) < 3:\n","            return False\n","        sequence = seq.tolist() + [next_token]\n","        # Check for repeating trigrams\n","        if len(sequence) >= 6:\n","            trigrams = [tuple(sequence[i:i+3]) for i in range(len(sequence)-2)]\n","            trigram_counts = Counter(trigrams)\n","            if max(trigram_counts.values()) > 1:\n","                return True\n","        # Check for repeating words\n","        token_counts = Counter(sequence)\n","        if max(token_counts.values()) > 2:\n","            return True\n","\n","        return False\n","\n","    def compute_word_tfidf_embedding(self, word, numberbatch_df, idf_values):\n","        \"\"\"\n","        Compute the TF-IDF embedding for a single word.\n","        \"\"\"\n","        if word in numberbatch_df.index:\n","            embedding = numberbatch_df.loc[word].values\n","            idf_value = idf_values.get(word, 1.0)\n","            return embedding * idf_value\n","        return np.zeros(numberbatch_df.shape[1])\n","\n","    def compute_caption_tfidf_embedding(self, words, numberbatch_df, idf_values):\n","      \"\"\"\n","      Compute the normalized TF-IDF-weighted embedding for a caption.\n","      \"\"\"\n","      embeddings = []\n","      weights = []\n","      for word in words:\n","          embedding = self.compute_word_tfidf_embedding(word, numberbatch_df, idf_values)\n","          if np.any(embedding):\n","              embeddings.append(embedding)\n","              weights.append(idf_values.get(word, 1.0))\n","\n","      if not embeddings:\n","          return np.zeros(numberbatch_df.shape[1])  # Return a zero vector if no embeddings are valid\n","\n","      embeddings = np.array(embeddings)\n","      weights = np.array(weights)\n","      normalized_embedding = np.sum(embeddings, axis=0) / (np.sum(weights) + 1e-8)  # Avoid division by zero\n","      return normalized_embedding"],"metadata":{"id":"DZ__uUOfvlSd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load all Embeddings and Setup\n"],"metadata":{"id":"T2BhqgWX-uH_"}},{"cell_type":"markdown","source":["We wil, be loading the vocaulary we saved previously and adding the special tokens for generation to it. After this, there should be 18809 elements in the vocabulary dict."],"metadata":{"id":"BiiE1l54ZsUs"}},{"cell_type":"code","source":["vocabulary_path = os.path.join(your_path, f\"{project_folder}/COCO/our_vocabulary.pkl\")\n","with open(vocabulary_path, 'rb') as f:\n","    word_to_idx = pickle.load(f)\n","\n","print(\"Vocabulary loaded!\")\n","print(len(word_to_idx))\n","word_to_idx[\"<PAD>\"] = len(word_to_idx)\n","word_to_idx[\"<UNK>\"] = len(word_to_idx)\n","word_to_idx[\"<s>\"] = len(word_to_idx)\n","word_to_idx[\"</s>\"] = len(word_to_idx)\n","print(len(word_to_idx))\n","idx_to_word = {idx: word for word, idx in word_to_idx.items()}"],"metadata":{"id":"duH-C5Guspnf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_token(token):\n","        return token.strip(string.punctuation).lower()\n","def encode_caption(caption, vocab, max_length=20):\n","    words = caption.split(\" \")\n","    indices = [vocab[\"<s>\"]] + [vocab.get(normalize_token(word), vocab[\"<UNK>\"]) for word in words] + [vocab[\"</s>\"]]\n","    if len(indices) > max_length:\n","        indices = indices[:max_length]\n","    else:\n","        indices += [vocab[\"<PAD>\"]] * (max_length - len(indices))\n","    return indices\n","\n","def decode_caption(indices, idx_to_word):\n","    words = []\n","    for idx in indices:\n","        word = idx_to_word.get(idx, \"<UNK>\")\n","        if word == \"</s>\":\n","            break\n","        words.append(word)\n","    return \" \".join(words[1:])"],"metadata":{"id":"mzKBHNidt9q1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# hyperparams\n","clip_dim = 512\n","kg_dim = 300\n","hidden_dim = 256\n","vocab_size = len(word_to_idx)\n","batch_size = 32\n","num_epochs = 75\n","learning_rate = 1e-4\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"UhNixgXm7rCQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_kg = LightweightCaptioningModel(clip_dim, kg_dim, hidden_dim, vocab_size, use_kg=True).to(device)\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=18805)\n","optimizer = optim.Adam(model_kg.parameters(), lr=learning_rate)"],"metadata":{"id":"5fskU4WuvekA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data\n","coco_embeddings_path = os.path.join(your_path, f\"{project_folder}/COCO/coco_embeddings.npz\")\n","idf_values_path = os.path.join(your_path, f\"{project_folder}/COCO/idf_values.csv\")\n","numberbatch_embeddings_path = os.path.join(your_path, f'{project_folder}/ConceptNet_Data_Container/numberbatch-en-19.08.txt.gz')\n","\n","coco_data = np.load(coco_embeddings_path, allow_pickle=True)\n","coco_image_features = torch.tensor(coco_data[\"image_features\"]).float().to(device)\n","coco_text_features = torch.tensor(coco_data[\"text_features\"]).float().to(device)\n","coco_captions = coco_data[\"text_captions\"]  # Use tokenized captions\n","coco_image_ids = coco_data[\"image_ids\"]"],"metadata":{"id":"yscnC73avx_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kg_coco_embedding_path = os.path.join(your_path, f\"{project_folder}/COCO/kg_coco_embeddings.npz\")\n","kg_coco_embeddings = np.load(kg_coco_embedding_path)\n","kg_embeddings = kg_coco_embeddings[\"kg_embeddings\"]"],"metadata":{"id":"PhJtDYJ05MH0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We filter out the duplicated image embeddings as each image occurs mutliple times in the set paired with different reference captions."],"metadata":{"id":"qaJc1FJS9Zbg"}},{"cell_type":"code","source":["kg_caption_to_index = defaultdict(list)\n","for idx, caption in enumerate(kg_coco_embeddings[\"captions\"]):\n","    kg_caption_to_index[caption].append(idx)\n","\n","unique_image_features = []\n","unique_text_features = []\n","unique_kg_embeddings = []\n","unique_captions = []\n","unique_image_ids = []\n","\n","for i, (image_id, caption) in enumerate(zip(coco_image_ids, coco_captions)):\n","    if image_id not in unique_image_ids:\n","        kg_indices = kg_caption_to_index.get(caption, [])\n","        if kg_indices:\n","            kg_embedding = kg_embeddings[kg_indices[0]]\n","        else:\n","            print(f\"No KG embedding found for caption: {caption}\")\n","            kg_embedding = np.zeros_like(kg_embeddings[0])\n","\n","        unique_image_features.append(coco_image_features[i])\n","        unique_text_features.append(coco_text_features[i])\n","        unique_kg_embeddings.append(kg_embedding)\n","        unique_captions.append(caption)\n","        unique_image_ids.append(image_id)\n","\n","unique_image_features = torch.stack(unique_image_features)\n","unique_text_features = torch.stack(unique_text_features)\n","unique_kg_embeddings = np.array(unique_kg_embeddings)\n","unique_captions = np.array(unique_captions, dtype=object)\n","unique_image_ids = np.array(unique_image_ids)\n","\n","print(unique_image_features.shape)\n","print(unique_text_features.shape)\n","print(unique_kg_embeddings.shape)\n","print(len(unique_captions))\n","print(len(unique_image_ids))"],"metadata":{"id":"Wqb8Cm7h9Yn7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filtered_coco_embeddings_path = os.path.join(your_path, f\"{project_folder}/COCO/filtered_coco_embeddings.npz\")"],"metadata":{"id":"WEsx5q6RliH1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.savez_compressed(\n","    filtered_coco_embeddings_path,\n","    image_features=unique_image_features.cpu().numpy(),\n","    text_features=unique_text_features.cpu().numpy(),\n","    kg_embeddings=unique_kg_embeddings,\n","    captions=unique_captions,\n","    image_ids=unique_image_ids\n",")\n","\n","print(f\"Saved filtered embeddings and captions to {filtered_coco_embeddings_path}\")"],"metadata":{"id":"jNJO83Oc_SyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_coco_embeddings = np.load(filtered_coco_embeddings_path, allow_pickle=True)\n","image_features = torch.tensor(all_coco_embeddings[\"image_features\"]).float().to(device)\n","text_features = torch.tensor(all_coco_embeddings[\"text_features\"]).float().to(device)\n","kg_embeddings = all_coco_embeddings[\"kg_embeddings\"]\n","coco_captions = all_coco_embeddings[\"captions\"]\n","image_ids = all_coco_embeddings[\"image_ids\"]"],"metadata":{"id":"j3VojrdM_sN0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"KHWgUAdn5I7A"}},{"cell_type":"code","source":["class CaptionDataset(Dataset):\n","    def __init__(self, clip_image_features, clip_text_features, kg_embeddings, captions, vocab, max_length=20):\n","        self.clip_image_features = clip_image_features\n","        self.clip_text_features = clip_text_features\n","        self.kg_embeddings = kg_embeddings\n","        self.captions = captions\n","        self.vocab = vocab\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.captions)\n","\n","    def __getitem__(self, idx):\n","        image_feature = torch.tensor(self.clip_image_features[idx], dtype=torch.float32)\n","        text_feature = torch.tensor(self.clip_text_features[idx], dtype=torch.float32)\n","        kg_embedding = torch.tensor(self.kg_embeddings[idx], dtype=torch.float32)\n","\n","        caption = encode_caption(self.captions[idx], self.vocab, self.max_length)\n","        caption = torch.tensor(caption, dtype=torch.long)\n","\n","        return image_feature, text_feature, kg_embedding, caption\n","\n","# Dataloader initilization\n","max_length = 20\n","dataset = CaptionDataset(\n","    clip_image_features=image_features.squeeze(1),  # CLIP image embeddings\n","    clip_text_features=text_features.squeeze(1),   # CLIP text embeddings\n","    kg_embeddings=kg_embeddings,        # Precomputed KG embeddings\n","    captions=coco_captions,             # Captions\n","    vocab=word_to_idx,                  # Vocabulary\n","    max_length=max_length               # Maximum length for padding\n",")\n","data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"],"metadata":{"id":"TGiMzjgS4lOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, data_loader, criterion, optimizer, num_epochs, device):\n","    model.train()\n","    losses = []\n","\n","    for epoch in range(num_epochs):\n","        epoch_loss = 0\n","        for batch in tqdm(data_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n","            clip_image_features, clip_text_features, kg_embeddings, captions = batch\n","            clip_image_features = clip_image_features.to(device)\n","            clip_text_features = clip_text_features.to(device)\n","            kg_embeddings = kg_embeddings.to(device)\n","            captions = captions.to(device)\n","\n","            lengths = (captions != word_to_idx[\"<PAD>\"]).sum(dim=1).cpu()\n","\n","            optimizer.zero_grad()\n","            logits, target_captions = model(\n","                clip_image_features,\n","                kg_embedding=kg_embeddings,\n","                captions=captions,\n","                lengths=lengths\n","            )\n","\n","            # Pad logits to match target sequence length\n","            target_len = target_captions.size(1)\n","            batch_size, seq_len, vocab_size = logits.shape\n","            padding_size = target_len - seq_len\n","\n","            if padding_size > 0:\n","                # Create padding\n","                padding = torch.zeros(batch_size, padding_size, vocab_size, device=device)\n","                # Set padding token probability to 1 in padding positions\n","                padding[:, :, word_to_idx[\"<PAD>\"]] = 1\n","                # Concatenate with original logits\n","                logits = torch.cat([logits, padding], dim=1)\n","\n","            # Reshape for loss calculation\n","            logits = logits.reshape(-1, vocab_size)\n","            target_captions = target_captions.reshape(-1)\n","\n","            # Create mask for non-padding tokens\n","            mask = target_captions != word_to_idx[\"<PAD>\"]\n","            logits = logits[mask]\n","            target_captions = target_captions[mask]\n","\n","            loss = criterion(logits, target_captions)\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(data_loader)}\")\n","        losses.append(epoch_loss / len(data_loader))\n","\n","    return losses"],"metadata":{"id":"ls47TB-z_R3x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train model\n","losses = train(model_kg, data_loader, criterion, optimizer, num_epochs=num_epochs, device=\"cuda\")"],"metadata":{"id":"gXn9ibMa-OVA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# save trained model\n","torch.save(model_kg.state_dict(), os.path.join(your_path, f\"path/to/model.pth\"))"],"metadata":{"id":"zWfFZNdSTPMf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(losses)\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Training Loss Over Epochs\")\n","plt.show()"],"metadata":{"id":"0H3CjBkdowC0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"SEU_WhfdnDUk"}},{"cell_type":"markdown","source":["## Model Loading"],"metadata":{"id":"LCCtWxpSgkob"}},{"cell_type":"code","source":["model_kg = LightweightCaptioningModel(clip_dim, kg_dim, hidden_dim, vocab_size, use_kg=True).to(device)\n","model_kg.load_state_dict(torch.load(os.path.join(your_path, f\"path/to/model.pth\")))"],"metadata":{"id":"-U1GI5wSdWZh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Loading all Embeddings"],"metadata":{"id":"_K6eLJPwgmib"}},{"cell_type":"code","source":["numberbatch_embeddings_path = os.path.join(your_path, f'{project_folder}/ConceptNet_Data_Container/numberbatch-en-19.08.txt.gz')\n","\n","def load_numberbatch_embeddings(file_path):\n","    embeddings = []\n","    terms = []\n","\n","    with gzip.open(file_path, 'rt', encoding='utf8') as f:\n","        next(f)\n","        for line in f:\n","            elements = line.strip().split()\n","            term = elements[0]\n","            vector = list(map(float, elements[1:]))\n","            terms.append(term)\n","            embeddings.append(vector)\n","\n","    return pd.DataFrame(embeddings, index=terms)\n","\n","\n","numberbatch_df = load_numberbatch_embeddings(numberbatch_embeddings_path)\n","\n","print(f\"Loaded {len(numberbatch_df)} embeddings.\")\n","print(numberbatch_df.head())"],"metadata":{"id":"CTJWdNwGl-L1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idf_values_path = os.path.join(your_path, f\"{project_folder}/Viz-Wiz Captions/idf_values.csv\")\n","def load_idf(idf_path):\n","  idf_df = pd.read_csv(idf_path)\n","  idf_dict = dict(zip(idf_df['word'], idf_df['idf']))\n","  return idf_dict\n","idf_vals = load_idf(idf_values_path)"],"metadata":{"id":"g5KXmTHmaZy1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up testing dataloader\n","viz_wiz_images_dir = f\"{project_folder}/Viz-Wiz Captions/val\"\n","viz_wiz_annotations_path = f\"{project_folder}/Viz-Wiz Captions/val.json\"\n","\n","total_wiz_images_dir = os.path.join(your_path, viz_wiz_images_dir)\n","total_wiz_annotations_path = os.path.join(your_path, viz_wiz_annotations_path)\n","\n","with open(total_wiz_annotations_path, 'r') as f:\n","    viz_wiz_data = json.load(f)\n","\n","images = {img['id']: img for img in viz_wiz_data['images']}\n","annotations = viz_wiz_data['annotations']\n","print(f\"Loaded {len(images)} images and {len(annotations)} annotations.\")"],"metadata":{"id":"37bIvhHYnkOE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kg_viz_wiz_embedding_path = os.path.join(your_path, f\"{project_folder}/Viz-Wiz Captions/kg_viz_wiz_embeddings.npz\")\n","kg_viz_wiz_embeddings = np.load(kg_viz_wiz_embedding_path)\n","kg_embeddings = kg_viz_wiz_embeddings[\"kg_embeddings\"]"],"metadata":{"id":"ns0Q4sfVok2u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that if you use our precomputed embeddings for `viz_wiz_embddings_path`, ignore the first 5004 entries (they are erroneous); you can do this by slicing [5004:] on each array."],"metadata":{"id":"HsePEvg3dkfr"}},{"cell_type":"code","source":["viz_wiz_embeddings_path = os.path.join(your_path, f\"{project_folder}/Viz-Wiz Captions/viz_wiz_embeddings.npz\")\n","viz_wiz_data = np.load(viz_wiz_embeddings_path, allow_pickle=True)\n","viz_wiz_image_features = torch.tensor(viz_wiz_data[\"image_features\"][5004:]).float().to(device)\n","viz_wiz_text_features = torch.tensor(viz_wiz_data[\"text_features\"][5004:]).float().to(device)\n","viz_wiz_captions = viz_wiz_data[\"text_captions\"][5004:] # Use tokenized captions\n","viz_wiz_image_ids = viz_wiz_data[\"image_ids\"][5004:]"],"metadata":{"id":"4prEIPF_onQR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset Setup"],"metadata":{"id":"v8wQWI5VgsL4"}},{"cell_type":"code","source":["class GroupedCaptionDataset(Dataset):\n","    def __init__(self, clip_image_features, captions, vocab, img_ids, max_length=20):\n","        self.clip_image_features = clip_image_features\n","        self.captions = captions\n","        self.vocab = vocab\n","        self.max_length = max_length\n","        self.num_captions_per_image = 5\n","        self.check_img_ordering(img_ids)\n","        self.img_ids = img_ids\n","\n","    def __len__(self):\n","        return len(self.captions) // self.num_captions_per_image\n","\n","    def __getitem__(self, idx):\n","        start_idx = idx * self.num_captions_per_image\n","        indices = range(start_idx, start_idx + self.num_captions_per_image)\n","\n","        # Take only first instance of image features since they're identical\n","        image_feature = torch.tensor(self.clip_image_features[start_idx], dtype=torch.float32).unsqueeze(0)\n","        img_ids = torch.tensor(self.img_ids[start_idx], dtype=torch.int32).unsqueeze(0)\n","\n","        # Stack all 5 captions\n","        captions = torch.stack([torch.tensor(encode_caption(self.captions[i], self.vocab, self.max_length), dtype=torch.long) for i in indices])\n","\n","        return image_feature, captions, img_ids\n","\n","    def check_img_ordering(self, array):\n","        if len(array) % 5 != 0:\n","            print(\"len( array)\", len(array))\n","            raise ValueError(\"The array length is not divisible by 5.\")\n","        else:\n","            reshaped = array.reshape(-1, 5)\n","            # Check if all numbers in each group are the same\n","            valid = np.all(reshaped[:, 0] == reshaped.T)\n","            if valid:\n","              return True\n","            else:\n","                raise ValueError(\"The array does not satisfy the condition.\")"],"metadata":{"id":"ef1AdbO8Qz8h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = GroupedCaptionDataset(\n","    clip_image_features=viz_wiz_image_features.squeeze(1),\n","    captions=viz_wiz_captions,\n","    vocab=word_to_idx,\n","    img_ids=viz_wiz_image_ids,\n","    max_length=20\n",")\n","test_data_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"oPN3i2qGovK7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inference"],"metadata":{"id":"sd1AHjXFgwOm"}},{"cell_type":"code","source":["def evaluate(model, data_loader, vocab, device, metrics=['bleu']):\n","    model.eval()\n","    all_predictions = []  # Will store (img_id, prediction) tuples\n","    all_references = []   # Will store (img_id, [references]) tuples\n","    criterion = nn.CrossEntropyLoss(ignore_index=18805)\n","    counter = 0\n","    total_samples_used = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n","            clip_image_features, captions, img_ids = batch\n","            clip_image_features = clip_image_features.to(device)\n","            captions = captions.to(device)\n","\n","            # Initialize the mask\n","            batch_size = captions.size(0)\n","            mask = torch.ones(batch_size, dtype=torch.bool)\n","\n","            # Check each batch\n","            for i in range(batch_size):\n","                for j in range(captions.size(1)):\n","                    caption_tokens = captions[i, j]\n","                    decoded_caption = decode_caption(caption_tokens.tolist(), vocab)\n","                    if \"quality issue\" in decoded_caption.lower():\n","                        mask[i] = False\n","                        break\n","\n","            # Apply the mask to filter batches\n","            captions = captions[mask]\n","            clip_image_features = clip_image_features[mask]\n","            # Keep track of total samples actually used\n","            total_samples_used += captions.size(0)\n","\n","            # Get model predictions\n","            outputs = model(clip_image_features.squeeze(1),\n","                          idx_to_word=idx_to_word,\n","                          numberbatch_df=numberbatch_df,\n","                          idf_values=idf_vals,\n","                          word_to_idx=word_to_idx)\n","\n","            for pred_seq, ref_captions, img_id in zip(outputs, captions, img_ids):\n","                # Convert prediction (skip special tokens but keep spaces between words)\n","                pred_tokens = []\n","                for token_idx in pred_seq:\n","                    token = idx_to_word[token_idx.item()]\n","                    if token not in [\"<PAD>\", \"<s>\", \"</s>\", \"<UNK>\"]:\n","                        pred_tokens.append(token)\n","\n","                # Store prediction with image ID\n","                if pred_tokens:\n","                    all_predictions.append((str(img_id.item()), \" \".join(pred_tokens))) # We make the img_id a string for CIDER and SPICE\n","                else:\n","                    print(\"Warning: Empty prediction sequence\")\n","                    all_predictions.append((str(img_id.item()), \"<EMPTY>\"))\n","\n","                # Format references\n","                refs = []\n","                for ref_caption in ref_captions:\n","                    ref_tokens = []\n","                    for token_idx in ref_caption:\n","                        token = idx_to_word[token_idx.item()]\n","                        if token not in [\"<PAD>\", \"<s>\", \"</s>\", \"<UNK>\"]:\n","                            ref_tokens.append(token)\n","                    if ref_tokens:  # Only add non-empty references\n","                        refs.append(\" \".join(ref_tokens))\n","                all_references.append((str(img_id.item()), refs))\n","            counter +=1\n","\n","    # Debug prints\n","    print(f\"\\nNumber of predictions: {len(all_predictions)}\")\n","    print(f\"Number of references: {len(all_references)}\")\n","    print(\"\\nSample predictions:\")\n","    for i in range(min(5, len(all_predictions))):\n","        print(f\"Image ID {all_predictions[i][0]}:\")\n","        print(f\"Prediction: '{all_predictions[i][1]}'\")\n","        print(f\"References: {all_references[i][1]}\\n\")\n","    return total_samples_used, all_predictions, all_references"],"metadata":{"id":"PipBu-Kx58zk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_samples_used, eval_all_predictions, eval_all_references = evaluate(model_kg, test_data_loader, idx_to_word, device)"],"metadata":{"id":"2TFozw7Wt4se"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare references - each item should be a list of word lists (multiple references per example)\n","non_tuple_references = [[r.split() for r in ref[-1]] for ref in eval_all_references]\n","print(\"References\", non_tuple_references[0])\n","\n","# Prepare candidates - each item should be a list of words\n","non_tuple_candidates = [pred[-1].split() for pred in eval_all_predictions]\n","print(f\"Candidate: {non_tuple_candidates[0]}\")"],"metadata":{"id":"EoznlMdi6YqU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluation Metrics"],"metadata":{"id":"qJ8m895mggwr"}},{"cell_type":"code","source":["# BLEU-1 Calculation Function\n","def calculate_bleu1(references, candidates):\n","    weights = (1.0, 0, 0, 0)\n","    smoothing_fn = SmoothingFunction().method1\n","    return corpus_bleu(references, candidates, weights=weights, smoothing_function=smoothing_fn)\n","\n","# ROUGE-L Calculation Function\n","def calculate_rouge(references, candidates):\n","    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n","    rouge_scores = []\n","    for refs, cand in zip(references, candidates):\n","        score = scorer.score(\" \".join(refs[0]), \" \".join(cand))\n","        rouge_scores.append(score[\"rougeL\"].fmeasure)\n","    return rouge_scores\n","\n","# Bootstrap for Confidence Intervals\n","def single_bootstrap(metric_fn, references, candidates):\n","    indices = np.random.choice(len(candidates), len(candidates), replace=True)\n","    sampled_references = [references[i] for i in indices]\n","    sampled_candidates = [candidates[i] for i in indices]\n","    return metric_fn(sampled_references, sampled_candidates)\n","\n","def bootstrap_metric_parallel(metric_fn, references, candidates, n_bootstraps=500, n_jobs=6):\n","    with Pool(n_jobs) as pool:\n","        scores = pool.starmap(\n","            single_bootstrap,\n","            [(metric_fn, references, candidates) for _ in range(n_bootstraps)]\n","        )\n","    mean_score = np.mean(scores)\n","    lower_ci = np.percentile(scores, 2.5)\n","    upper_ci = np.percentile(scores, 97.5)\n","    return mean_score, lower_ci, upper_ci\n","\n","# BLEU-1\n","bleu_mean, bleu_ci_lower, bleu_ci_upper = bootstrap_metric_parallel(calculate_bleu1, non_tuple_references, non_tuple_candidates)\n","bleu_results = {\"mean\": bleu_mean, \"95% CI\": (bleu_ci_lower, bleu_ci_upper)}\n","print(f\"with kg BLEU-1: {bleu_mean:.4f} (95% CI: {bleu_ci_lower:.4f}, {bleu_ci_upper:.4f})\")\n","\n","# ROUGE-L\n","rouge_scores = calculate_rouge(non_tuple_references, non_tuple_candidates)\n","rouge_mean = np.mean(rouge_scores)\n","rouge_ci_lower = np.percentile(rouge_scores, 2.5)\n","rouge_ci_upper = np.percentile(rouge_scores, 97.5)\n","rouge_results = {\"mean\": rouge_mean, \"95% CI\": (rouge_ci_lower, rouge_ci_upper)}\n","print(f\"with kg ROUGE-L: {rouge_mean:.4f} (95% CI: {rouge_ci_lower:.4f}, {rouge_ci_upper:.4f})\")"],"metadata":{"id":"_fV1e5xNI2Yv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate stored model with no KG for comparison tests below\n","model_no_kg = LightweightCaptioningModel(clip_dim, kg_dim, hidden_dim, vocab_size, use_kg=False).to(device)\n","model_no_kg.load_state_dict(torch.load(os.path.join(your_path, f\"path/to/model.pth\")))\n","CTRL_total_samples_used, CTRL_eval_all_predictions, CTRL_eval_all_references = evaluate(model_no_kg, test_data_loader, idx_to_word, device)"],"metadata":{"id":"qQOfR3p9L4UQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare references - each item should be a list of word lists (multiple references per example)\n","CTRL_non_tuple_references = [[r.split() for r in ref[-1]] for ref in CTRL_eval_all_references]\n","print(\"References\", CTRL_non_tuple_references[0])\n","\n","# Prepare candidates - each item should be a list of words\n","CTRL_non_tuple_candidates = [pred[-1].split() for pred in CTRL_eval_all_predictions]\n","print(f\"Candidate: {CTRL_non_tuple_candidates[0]}\")"],"metadata":{"id":"2h9D0KnBMEB7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BLEU-1\n","bleu_mean, bleu_ci_lower, bleu_ci_upper = bootstrap_metric_parallel(calculate_bleu1, CTRL_non_tuple_references, CTRL_non_tuple_candidates)\n","bleu_results = {\"mean\": bleu_mean, \"95% CI\": (bleu_ci_lower, bleu_ci_upper)}\n","print(f\"no kg BLEU-1: {bleu_mean:.4f} (95% CI: {bleu_ci_lower:.4f}, {bleu_ci_upper:.4f})\")\n","\n","# ROUGE-L\n","rouge_scores = calculate_rouge(CTRL_non_tuple_references, CTRL_non_tuple_candidates)\n","rouge_mean = np.mean(rouge_scores)\n","rouge_ci_lower = np.percentile(rouge_scores, 2.5)\n","rouge_ci_upper = np.percentile(rouge_scores, 97.5)\n","rouge_results = {\"mean\": rouge_mean, \"95% CI\": (rouge_ci_lower, rouge_ci_upper)}\n","print(f\"no kg ROUGE-L: {rouge_mean:.4f} (95% CI: {rouge_ci_lower:.4f}, {rouge_ci_upper:.4f})\")"],"metadata":{"id":"6GGIvhj7Mm2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save values for easy comparison\n","bleu1_with_kg = [calculate_bleu1([non_tuple_references[i]], [non_tuple_candidates[i]]) for i in range(len(non_tuple_candidates))]\n","bleu1_without_kg = [calculate_bleu1([non_tuple_references[i]], [CTRL_non_tuple_candidates[i]]) for i in range(len(CTRL_non_tuple_candidates))]\n","rouge_with_kg = calculate_rouge(non_tuple_references, non_tuple_candidates)\n","rouge_without_kg = calculate_rouge(non_tuple_references, CTRL_non_tuple_candidates)"],"metadata":{"id":"_FPjoq3DMydJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_distribution(group1, group2):\n","    plt.figure(figsize=(4, 4))\n","    n_bins = len(np.unique(np.concatenate([group1, group2])))\n","\n","    plt.hist(group1, bins=n_bins, alpha=0.5, density=True,\n","             label='With KG', color='blue')\n","    plt.hist(group2, bins=n_bins, alpha=0.5, density=True,\n","             label='Without KG', color='red')\n","\n","    plt.axvline(np.mean(group1), color='blue', linestyle='--')\n","    plt.axvline(np.mean(group2), color='red', linestyle='--')\n","\n","    # Calculate Cohen's d\n","    n1, n2 = len(group1), len(group2)\n","    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n","    pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2))\n","    cohens_d = (np.mean(group1) - np.mean(group2)) / pooled_std\n","\n","    plt.title(\"Score Distribution\")\n","    plt.xlabel('Score')\n","    plt.ylabel('Density')\n","    plt.grid(True, alpha=0.3)\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"zXL-UgmkV-gN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# BLEU-1\n","plot_distribution(bleu1_with_kg, bleu1_without_kg)"],"metadata":{"id":"cXxkEmlO0qXZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ROUGE\n","plot_distribution(rouge_with_kg, rouge_without_kg)"],"metadata":{"id":"66mZM-L70sPg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gts = {kc[0]: kc[1] for kc in eval_all_references}\n","res = {kc[0]: [kc[1]] for kc in eval_all_predictions}"],"metadata":{"id":"sfeiZO9mFHv3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate CIDEr\n","cider_scorer = Cider()\n","cider_score, _ = cider_scorer.compute_score(gts, res)\n","print(f\"CIDEr Score: {cider_score}\")\n","\n","# Calculate SPICE\n","spice_scorer = Spice()\n","spice_score, _ = spice_scorer.compute_score(gts, res)\n","print(f\"SPICE Score: {spice_score}\")"],"metadata":{"id":"2IAYvNswFomu"},"execution_count":null,"outputs":[]}]}